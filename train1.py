"""
train1.py â€” ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GazeNet Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ MPIIGaze.

Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ: Leave-One-Person-Out (LOPO) â€” Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° p00-p13, Ñ‚ĞµÑÑ‚ Ğ½Ğ° p14.
Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ: Angular Loss (ÑƒĞ³Ğ»Ğ¾Ğ²Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² Ñ€Ğ°Ğ´Ğ¸Ğ°Ğ½Ğ°Ñ…).
ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€: AdamW Ñ warmup 3 ÑĞ¿Ğ¾Ñ…Ğ¸ + cosine decay.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import time
import random
import os
import matplotlib
matplotlib.use('Agg')   # Ğ±ĞµĞ· GUI â€” Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ² Ñ„Ğ°Ğ¹Ğ»
import matplotlib.pyplot as plt

from dataset_loader import GazeDataset
from test_model import GazeNet

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_PATH     = r'C:\Users\User\Desktop\def\ai test\1\MPIIGaze\Data\Normalized'
MODEL_SAVE    = "gaze_model_lopo.pth"   # Ñ„Ğ°Ğ¹Ğ» Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ²ĞµÑĞ¾Ğ²

TRAIN_IDS     = [f'p{i:02d}' for i in range(14)]  # p00 - p13
TEST_ID       = ['p14']                            # Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ°

BATCH_SIZE    = 256    # Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ±Ğ°Ñ‚Ñ‡ = ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ĞµĞµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹
LEARNING_RATE = 1e-3   # ÑÑ‚Ğ°Ñ€Ñ‚ÑƒĞµĞ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾, scheduler Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ Ğ´Ğ¾ 1e-6
EPOCHS        = 50
DEVICE        = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Ğ’Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
torch.backends.cudnn.benchmark = True
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

print(f"Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: {DEVICE}", flush=True)
print(f"ĞĞ±ÑƒÑ‡Ğ°ĞµĞ¼ Ğ½Ğ°: {len(TRAIN_IDS)} Ñ‡ĞµĞ»., Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ½Ğ°: {TEST_ID}", flush=True)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ANGULAR LOSS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def pitchyaw_to_vector(pitchyaw):
    """
    ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ³Ğ»Ñ‹ (pitch, yaw) Ğ² 3D ĞµĞ´Ğ¸Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğ°.

    Args:
        pitchyaw (torch.Tensor): Ğ¢ĞµĞ½Ğ·Ğ¾Ñ€ (B, 2) â€” [pitch, yaw] Ğ² Ñ€Ğ°Ğ´Ğ¸Ğ°Ğ½Ğ°Ñ….

    Returns:
        torch.Tensor: Ğ¢ĞµĞ½Ğ·Ğ¾Ñ€ (B, 3) â€” ĞµĞ´Ğ¸Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ [x, y, z].
    """
    pitch = pitchyaw[:, 0]
    yaw   = pitchyaw[:, 1]
    x = -torch.cos(pitch) * torch.sin(yaw)
    y = -torch.sin(pitch)
    z = -torch.cos(pitch) * torch.cos(yaw)
    return torch.stack((x, y, z), dim=1)


def angular_loss(pred, label):
    """
    Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ ÑƒĞ³Ğ»Ğ¾Ğ²ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ² Ñ€Ğ°Ğ´Ğ¸Ğ°Ğ½Ğ°Ñ….

    ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ pitch/yaw Ğ² 3D Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚, ÑÑ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ arccos dot product.
    Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ MSE â€” ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ ÑƒĞ³Ğ»Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ğ¼Ğ¸.

    Args:
        pred  (torch.Tensor): ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (B, 2).
        label (torch.Tensor): ĞœĞµÑ‚ĞºĞ¸ (B, 2).

    Returns:
        torch.Tensor: Ğ¡ĞºĞ°Ğ»ÑÑ€Ğ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ ÑƒĞ³Ğ»Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ñ€Ğ°Ğ´Ğ¸Ğ°Ğ½Ğ°Ñ….
    """
    pred_vec  = torch.nn.functional.normalize(pitchyaw_to_vector(pred),  dim=1)
    label_vec = torch.nn.functional.normalize(pitchyaw_to_vector(label), dim=1)
    # Clamp Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ NaN Ğ² arccos Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑÑ… Ñ€Ğ¾Ğ²Ğ½Ğ¾ Â±1
    dot = torch.sum(pred_vec * label_vec, dim=1).clamp(-1.0 + 1e-6, 1.0 - 1e-6)
    return torch.mean(torch.acos(dot))


def compute_angular_error(pred, label):
    """
    Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ ÑƒĞ³Ğ»Ğ¾Ğ²ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ² Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ°Ñ… (Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ).

    Args:
        pred  (torch.Tensor): ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (B, 2).
        label (torch.Tensor): ĞœĞµÑ‚ĞºĞ¸ (B, 2).

    Returns:
        float: Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ ÑƒĞ³Ğ»Ğ¾Ğ²Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ°Ñ….
    """
    pred_vec  = torch.nn.functional.normalize(pitchyaw_to_vector(pred),  dim=1)
    label_vec = torch.nn.functional.normalize(pitchyaw_to_vector(label), dim=1)
    dot = torch.sum(pred_vec * label_vec, dim=1).clamp(-1.0 + 1e-6, 1.0 - 1e-6)
    return torch.mean(torch.acos(dot) * (180 / np.pi)).item()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞĞ£Ğ“ĞœĞ•ĞĞ¢ĞĞ¦Ğ˜Ğ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def augment_batch(images):
    """
    ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ±Ğ°Ñ‚Ñ‡Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (Ğ½Ğ° GPU).

    ĞÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ: Ñ€Ğ°Ğ·Ğ½Ğ¾Ğµ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ, ĞºĞ°Ğ¼ĞµÑ€Ñ‹,
    Ñ€Ğ°ÑÑ„Ğ¾ĞºÑƒÑ. Ğ’ÑĞµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ÑÑ Ğ½Ğ° Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ°Ñ… Ğ±ĞµĞ· ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° CPU.

    Args:
        images (torch.Tensor): Ğ‘Ğ°Ñ‚Ñ‡ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (B, 1, 36, 60), [0, 1].

    Returns:
        torch.Tensor: ĞÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±Ğ°Ñ‚Ñ‡ (B, 1, 36, 60), [0, 1].
    """
    B = images.size(0)

    # Ğ¯Ñ€ĞºĞ¾ÑÑ‚ÑŒ Â±40% â€” Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğµ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ
    brightness = torch.empty(B, 1, 1, 1, device=images.device).uniform_(0.6, 1.4)
    images = images * brightness

    # Ğ¡Ğ´Ğ²Ğ¸Ğ³ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ° â€” Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹
    contrast = torch.empty(B, 1, 1, 1, device=images.device).uniform_(-0.15, 0.15)
    images = images + contrast

    # Ğ“Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ğ¹ ÑˆÑƒĞ¼ â€” Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑˆÑƒĞ¼ ÑĞµĞ½ÑĞ¾Ñ€Ğ° ĞºĞ°Ğ¼ĞµÑ€Ñ‹
    noise = torch.randn_like(images) * 0.03
    images = images + noise

    # Ğ“Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ğµ (30% ÑˆĞ°Ğ½Ñ) â€” Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑÑ„Ğ¾ĞºÑƒÑ
    if random.random() < 0.3:
        kernel = torch.ones(1, 1, 1, 3, device=images.device) / 3
        images = torch.nn.functional.conv2d(images, kernel, padding=(0, 1), groups=1)

    return images.clamp(0, 1)  # Ğ¾Ğ±Ñ€ĞµĞ·Ğ°ĞµĞ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ [0, 1]


def augment_with_flip(images, labels):
    """
    Ğ¡Ğ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¹ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ğ»Ğ¸Ğ¿ Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ¼ĞµÑ‚ĞºĞ¸ yaw.

    ĞŸÑ€Ğ¸ Ğ·ĞµÑ€ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ³Ğ»Ğ°Ğ·Ğ° Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑƒĞ³Ğ¾Ğ» Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğ°
    Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ·Ğ½Ğ°Ğº: yaw -> -yaw. Ğ‘ĞµĞ· ÑÑ‚Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ
    Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞ»Ğ° Ğ±Ñ‹ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹.

    Args:
        images (torch.Tensor): Ğ‘Ğ°Ñ‚Ñ‡ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (B, 1, 36, 60).
        labels (torch.Tensor): Ğ‘Ğ°Ñ‚Ñ‡ Ğ¼ĞµÑ‚Ğ¾Ğº (B, 2) â€” [pitch, yaw].

    Returns:
        tuple[torch.Tensor, torch.Tensor]: ĞÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ images Ğ¸ labels.
    """
    B = images.size(0)
    flip_mask = torch.rand(B) < 0.3  # 30% Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼

    images[flip_mask] = torch.flip(images[flip_mask], dims=[3])  # flip Ğ¿Ğ¾ ÑˆĞ¸Ñ€Ğ¸Ğ½Ğµ
    labels[flip_mask, 1] = -labels[flip_mask, 1]  # yaw Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ·Ğ½Ğ°Ğº

    return images, labels


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ•
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def train_cross_subject():
    """
    ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GazeNet.

    Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ LOPO: p00-p13 Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, p14 Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ°.
    Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ²ĞµÑĞ° Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ.
    """
    print("\n=== Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ Ğ”ĞĞĞĞ«Ğ¥ ===", flush=True)
    train_dataset = GazeDataset(DATA_PATH, TRAIN_IDS)
    test_dataset  = GazeDataset(DATA_PATH, TEST_ID)

    # shuffle=True Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ â€” Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ°Ğ¶Ğ´ÑƒÑ ÑĞ¿Ğ¾Ñ…Ñƒ
    # pin_memory=True ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° GPU
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,
                              num_workers=0, pin_memory=True)
    test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,
                              num_workers=0, pin_memory=True)

    print(f"Train: {len(train_dataset)}  |  Test: {len(test_dataset)}", flush=True)

    if len(train_dataset) == 0:
        print("âŒ Train Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿ÑƒÑÑ‚!", flush=True)
        return

    # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    model = GazeNet().to(DEVICE)
    print("ĞĞ±ÑƒÑ‡Ğ°ĞµĞ¼ Ñ Ğ½ÑƒĞ»Ñ", flush=True)

    # AdamW â€” Adam Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ weight decay (Ğ½Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğº Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñƒ)
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)

    def lr_lambda(epoch):
        """Warmup 3 ÑĞ¿Ğ¾Ñ…Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾, Ğ·Ğ°Ñ‚ĞµĞ¼ cosine decay Ğ´Ğ¾ eta_min."""
        warmup = 3
        if epoch < warmup:
            return (epoch + 1) / warmup  # Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¹ Ñ€Ğ¾ÑÑ‚ 0 -> 1
        progress = (epoch - warmup) / (EPOCHS - warmup)
        return 0.5 * (1 + np.cos(np.pi * progress))  # cosine decay 1 -> 0

    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

    # Ğ¡Ğ°Ğ½Ğ¸Ñ‚Ğ°Ñ€Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼
    dummy = torch.randn(2, 1, 36, 60).to(DEVICE)
    with torch.no_grad():
        test_out = model(dummy)
    print(f"Ğ¢ĞµÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: shape={test_out.shape}, NaN={torch.isnan(test_out).any()}", flush=True)
    print("======================\n", flush=True)

    best_error   = float('inf')  # Ğ»ÑƒÑ‡ÑˆĞ°Ñ ÑƒĞ³Ğ»Ğ¾Ğ²Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğµ
    no_improve   = 0             # ÑÑ‡Ñ‘Ñ‚Ñ‡Ğ¸Ğº ÑĞ¿Ğ¾Ñ… Ğ±ĞµĞ· ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ
    patience     = 10            # early stopping Ñ‡ĞµÑ€ĞµĞ· 10 ÑĞ¿Ğ¾Ñ… Ğ±ĞµĞ· ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹
    train_losses = []            # Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ train loss Ğ´Ğ»Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°
    test_errors  = []            # Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ test error Ğ´Ğ»Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°

    for epoch in range(EPOCHS):
        start = time.time()
        model.train()  # Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ dropout Ğ¸ batchnorm Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
        train_loss    = 0.0
        valid_batches = 0

        for images, labels in train_loader:
            images, labels = images.to(DEVICE), labels.to(DEVICE)

            # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ±Ğ°Ñ‚Ñ‡Ğ¸ Ñ NaN (Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½ÑƒÑ‚ÑŒ Ğ¸Ğ·-Ğ·Ğ° Ğ³Ñ€ÑĞ·Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)
            if torch.isnan(images).any() or torch.isnan(labels).any():
                continue

            # ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸
            images = augment_batch(images)
            images, labels = augment_with_flip(images, labels)

            optimizer.zero_grad()
            outputs = model(images)

            # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ±Ğ°Ñ‚Ñ‡ ĞµÑĞ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹Ğ´Ğ°Ğ»Ğ° NaN
            if torch.isnan(outputs).any():
                continue

            loss = angular_loss(outputs, labels)

            # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ĞµÑĞ»Ğ¸ loss Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ĞµĞ½
            if torch.isnan(loss) or torch.isinf(loss):
                continue

            loss.backward()
            # Gradient clipping â€” Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ¾ÑÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            train_loss    += loss.item()
            valid_batches += 1

        if valid_batches == 0:
            continue

        avg_train_loss = train_loss / valid_batches
        scheduler.step()  # Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ learning rate

        # â”€â”€ ĞĞ¦Ğ•ĞĞšĞ ĞĞ Ğ¢Ğ•Ğ¡Ğ¢ĞĞ’ĞĞœ Ğ£Ğ§ĞĞ¡Ğ¢ĞĞ˜ĞšĞ• â”€â”€
        model.eval()  # Ğ¾Ñ‚ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ dropout Ğ´Ğ»Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°
        test_error = 0.0
        batches    = 0

        with torch.no_grad():  # Ğ½Ğµ ÑÑ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ñ‚ĞµÑÑ‚Ğµ
            for images, labels in test_loader:
                images, labels = images.to(DEVICE), labels.to(DEVICE)
                outputs = model(images)
                err = compute_angular_error(outputs, labels)
                if not np.isnan(err):
                    test_error += err
                    batches    += 1

        avg_test_error = test_error / batches if batches > 0 else float('inf')
        lr_now = optimizer.param_groups[0]['lr']

        print(f"Epoch {epoch+1:02d}/{EPOCHS} | "
              f"Loss: {avg_train_loss:.4f} rad | "
              f"Test: {avg_test_error:.2f}Â° | "
              f"LR: {lr_now:.2e} | "
              f"Time: {time.time()-start:.1f}s", flush=True)

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ»Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°
        train_losses.append(avg_train_loss)
        test_errors.append(avg_test_error)

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ²ĞµÑĞ° ĞµÑĞ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ÑÑŒ
        if avg_test_error < best_error:
            best_error = avg_test_error
            no_improve = 0
            torch.save(model.state_dict(), MODEL_SAVE)
            print(f"  âœ“ Ğ›ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ° ({best_error:.2f}Â°)", flush=True)
        else:
            no_improve += 1
            if no_improve >= patience:
                print(f"â¹ Early stopping Ğ½Ğ° ÑĞ¿Ğ¾Ñ…Ğµ {epoch+1}", flush=True)
                break

    print(f"\nğŸ‰ Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾. Ğ›ÑƒÑ‡ÑˆĞ°Ñ ÑƒĞ³Ğ»Ğ¾Ğ²Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {best_error:.2f}Â°", flush=True)

    # â”€â”€ Ğ“Ğ ĞĞ¤Ğ˜Ğš ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ¯ â”€â”€
    epochs_range = list(range(1, len(train_losses) + 1))
    fig, ax1 = plt.subplots(figsize=(10, 6))

    # Train loss â€” Ğ»ĞµĞ²Ğ°Ñ Ğ¾ÑÑŒ (ÑĞ¸Ğ½ÑÑ)
    color_train = '#2196F3'
    ax1.set_xlabel('Ğ­Ğ¿Ğ¾Ñ…Ğ°', fontsize=13)
    ax1.set_ylabel('Train Loss (Ñ€Ğ°Ğ´)', color=color_train, fontsize=13)
    ax1.plot(epochs_range, train_losses, color=color_train, linewidth=2.5,
             marker='o', markersize=5, label='Train Loss')
    ax1.tick_params(axis='y', labelcolor=color_train)

    # Test error â€” Ğ¿Ñ€Ğ°Ğ²Ğ°Ñ Ğ¾ÑÑŒ (ĞºÑ€Ğ°ÑĞ½Ğ°Ñ)
    ax2 = ax1.twinx()
    color_test = '#F44336'
    ax2.set_ylabel('Test Angular Error (Â°)', color=color_test, fontsize=13)
    ax2.plot(epochs_range, test_errors, color=color_test, linewidth=2.5,
             marker='s', markersize=5, linestyle='--', label='Test Error')
    ax2.tick_params(axis='y', labelcolor=color_test)

    # Ğ’ĞµÑ€Ñ‚Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ»Ğ¸Ğ½Ğ¸Ñ Ğ½Ğ° Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑĞ¿Ğ¾Ñ…Ğµ
    best_ep  = test_errors.index(min(test_errors)) + 1
    best_val = min(test_errors)
    ax2.axvline(x=best_ep, color='green', linestyle=':', linewidth=1.5, alpha=0.7)
    ax2.annotate(f'Ğ›ÑƒÑ‡ÑˆĞ°Ñ ÑĞ¿Ğ¾Ñ…Ğ° {best_ep}\n{best_val:.2f}Â°',
                 xy=(best_ep, best_val), xytext=(best_ep + 0.4, best_val + 0.1),
                 fontsize=10, color='green',
                 arrowprops=dict(arrowstyle='->', color='green'))

    # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ»ĞµĞ³ĞµĞ½Ğ´Ñ‹ Ğ´Ğ²ÑƒÑ… Ğ¾ÑĞµĞ¹
    lines1, labels1 = ax1.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right', fontsize=11)

    plt.title('GazeNet: Train Loss Ğ¸ Test Angular Error Ğ¿Ğ¾ ÑĞ¿Ğ¾Ñ…Ğ°Ğ¼', fontsize=14)
    plt.xticks(epochs_range)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')
    plt.close()
    print("ğŸ“Š Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½: training_curves.png", flush=True)


if __name__ == "__main__":
    train_cross_subject()